ğŸ§  RAG Backend Blueprint â€” FastAPI + LangChain + FAISS

This repository is a blueprint for building Retrieval-Augmented Generation (RAG) systems with ease.
Itâ€™s not just another AI backend â€” itâ€™s a plug-and-play foundation showing exactly how to connect PDFs, embeddings, and chat logic into a clean, modular FastAPI architecture.

If youâ€™ve ever wanted to create your own custom knowledge-aware chatbot that actually remembers context and talks from your data, this is your roadmap.

ğŸš€ Highlights
ğŸ§© Modular Design â€” Each layer (config, services, routes, schemas) is cleanly separated
ğŸ“š RAG Workflow Ready â€” Upload â†’ Embed â†’ Retrieve â†’ Chat
âš™ï¸ Built with Best Practices â€” Pydantic settings, service layers, structured logging
ğŸ§  LLM-Powered Conversations â€” Uses Groq API with memory-aware context
ğŸ’¾ PDF to Vector Pipeline â€” Automatic embedding via FAISS and Ollama

ğŸ—ï¸ Project Overview
backend/
â”‚
â”œâ”€â”€ main.py                # FastAPI entry point
â”œâ”€â”€ config.py              # Centralized app configuration & environment management
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ routes.py          # API endpoints (upload, chat, sessions, health)
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ vector_service.py  # Handles PDF embedding, FAISS storage, and retrieval
â”‚   â””â”€â”€ chat_service.py    # Runs the RAG pipeline using Groq + LangChain
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ schemas.py         # Request/response models with validation
â”‚
â””â”€â”€ utils/
    â””â”€â”€ helpers.py         # Reusable utility functions

âš™ï¸ Setup Guide
1ï¸âƒ£ Clone and Navigate
git clone https://github.com/YourUsername/rag-backend-blueprint.git
cd rag-backend/backend

2ï¸âƒ£ Create a Virtual Environment
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Configure Environment
Create a .env file in the backend/ folder:
GROQ_API_KEY=your_groq_key
HF_TOKEN=your_huggingface_token
OLLAMA_BASE_URL=http://localhost:11434
EMBEDDING_MODEL=all-minilm
LLM_MODEL=llama3-70b

â–¶ï¸ Run the Server
uvicorn main:app --reload

Access API docs here:
ğŸ‘‰ http://127.0.0.1:8000/docs

ğŸ” API Overview
Endpoint	Method	Description
/health	GET	Check server, embeddings, and model status
/upload	POST	Upload a PDF and create its vectorstore
/chat	POST	Send a message and get an AI response
/sessions	GET	List all active sessions
/sessions/{id}/info	GET	Get session metadata
/sessions/{id}	DELETE	Delete a session
/sessions/{id}/clear-history	POST	Clear chat history but keep vectors

ğŸ§© Under the Hood
Upload PDFs â†’ split text â†’ embed â†’ store vectors
Ask Questions â†’ retrieve context â†’ send to Groq LLM
Generate Responses â†’ contextual answers with source references
Maintain Sessions â†’ each chat remembers its own history
This repo is built to be understandable first, powerful second.
You can extend it to multiple document types, new models, or databases with minimal refactoring.

ğŸ§  Tech Stack

Framework: FastAPI
Language: Python 3.10+
Vector DB: FAISS
LLM API: Groq
Embeddings: Ollama
Core Tools: LangChain, Pydantic, python-dotenv

ğŸš§ Future Add-Ons
Multi-document retrieval
Redis for caching and session memory
Streamlit / React front-end
User authentication and role management

ğŸ‘¨â€ğŸ’» Author
Ahmed Elhfnawi
AI / ML Engineer
